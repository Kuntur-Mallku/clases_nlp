{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c88e42-50d4-4b4d-bdf1-894bcff1a41d",
   "metadata": {},
   "source": [
    "# NLP and Neural Networks\n",
    "\n",
    "In this exercise, we'll apply our knowledge of neural networks to process natural language. As we did in the bigram exercise, the goal of this lab is to predict the next word, given the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132d376-54d7-48c3-a52c-ac3d94ed798b",
   "metadata": {},
   "source": [
    "### Data set\n",
    "\n",
    "Load the text from \"One Hundred Years of Solitude\" that we used in our bigrams exercise. It's located in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e309d79-7746-40e3-8a02-3cc7b45c16ac",
   "metadata": {},
   "source": [
    "### Important note:\n",
    "\n",
    "Start with a smaller part of the text. Maybe the first 10 parragraphs, as the number of tokens rapidly increases as we add more text. \n",
    "\n",
    "Later you can use a bigger corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "15432063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8fdaf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectamos con la direccion del contenedor del servidor\n",
    "os.chdir(\"/home/kmuenala/nlp/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9bbced32-a252-48b0-bc8f-cecfdcf1ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los 10 primeros parrafos del archivo chapter1.txt\n",
    "text = ''\n",
    "count = 0\n",
    "with open('chapter1.txt', 'r', encoding='utf-8') as archivo:\n",
    "    for linea in archivo:\n",
    "        text += linea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1393d6-7cbf-4e8e-a699-0f0cd28982a3",
   "metadata": {},
   "source": [
    "Don't forget to prepare the data by generating the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9c33da77-ad0b-4eeb-9eaa-0dc98485187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7e34e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681843a-18f0-4d7c-9b02-83015f4383e1",
   "metadata": {},
   "source": [
    "### Let's prepare the data set.\n",
    "\n",
    "Our neural network needs to have an input X and an output y. Remember that these sets are numerical, so you'd need something to map the tokens into numbers, and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c820ccde-c2ee-41ef-b840-41ccca58b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this case, let's consider a bigram (w1, w2)\n",
    "# assign the w1 to the X vector, and w2 to the y vector, why do we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6da5540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_tokens = sorted(list(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "622287bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttoi = {t:i for i,t in enumerate(uniq_tokens)}\n",
    "itot = {i:t for t,i in ttoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5c57dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tokens[:-1]\n",
    "W2 = tokens[1:]\n",
    "\n",
    "X = [ttoi.get(w1, w1) for w1 in W1]\n",
    "y = [ttoi.get(w2, w2) for w2 in W2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cfedfd3b-0396-456b-b9f9-f1c07262a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget that since we are using torch, our training set vectors should be tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ffe6e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5984fd00-bdbf-4403-a341-b7ef83138db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that our vectors are integers, which can be thought as a categorical variables.\n",
    "# torch provides the one_hot method, that would generate tensors suitable for our nn\n",
    "# make sure that the dtype of your tensor is float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "3fb437ef-01ed-4dfd-9d66-5d40f9b6f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = len(uniq_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "77030d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "X_one_hot = F.one_hot(X_tensor, num_classes=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8db6057a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  66,  130,   64,  ...,  956, 1200, 1228])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3a87830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_one_hot= X_one_hot.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda25114-c6ae-4e07-a743-12e10cd77796",
   "metadata": {},
   "source": [
    "### Network design\n",
    "To start, we are going to have a very simple network. Define a single layer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "82bfac7a-e670-4aaf-bf30-5aa8d0ca46e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many neurons should our input layer have?\n",
    "# Use as many neurons as the total number of categories (from your one-hot encoded tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "99c50d1e-3842-451e-8ede-1c68dddf3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)  # Una capa lineal\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = F.softmax(out, dim=1) # Aplicar softmax en la salida\n",
    "        return out\n",
    "\n",
    "modelo = SimpleNet(input_size=n_tokens, output_size=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "202a7281-9ebb-4234-b883-2185cb3bcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the softmax as your activation layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b62f0fdc-bfb8-4081-acfa-bcab5226b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelo.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "15600485-24e3-4716-92ed-28ac1aa792bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "eb8b0c9b-55b2-4397-8a0d-e4b13fd2fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: torch.Size([5206, 1962]), torch.Size([5206])\n",
      "Tamaño del conjunto de prueba: torch.Size([1302, 1962]), torch.Size([1302])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_one_hot, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {X_test.shape}, {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9eb4fd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 7.5783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100], Loss: 7.5436\n",
      "Epoch [30/100], Loss: 7.3709\n",
      "Epoch [40/100], Loss: 7.2706\n",
      "Epoch [50/100], Loss: 7.2338\n",
      "Epoch [60/100], Loss: 7.2063\n",
      "Epoch [70/100], Loss: 7.1549\n",
      "Epoch [80/100], Loss: 7.1484\n",
      "Epoch [90/100], Loss: 7.1470\n",
      "Epoch [100/100], Loss: 7.1463\n",
      "Entrenamiento completado\n"
     ]
    }
   ],
   "source": [
    "modelo.train()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = modelo(X_train)\n",
    "    \n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "print(\"Entrenamiento completado\")\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d09aa-8a47-4668-b1b1-5080be8851ed",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d06d9c5-4df1-4145-812f-ff86958154c1",
   "metadata": {},
   "source": [
    "1. Test your network with a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1ac920e4-e9fc-4760-b031-9c959d78bae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor de salida para los primeros ejemplos de prueba:\n",
      "tensor([[3.5165e-06, 9.9836e-08, 3.7724e-06,  ..., 3.7385e-06, 9.7503e-08,\n",
      "         2.4320e-06],\n",
      "        [7.5782e-06, 2.1742e-07, 8.1329e-06,  ..., 8.0918e-06, 2.1184e-07,\n",
      "         4.2950e-06]], grad_fn=<SliceBackward0>)\n",
      "Categorías predichas para los primeros ejemplos de prueba:\n",
      "tensor([1207,  852])\n"
     ]
    }
   ],
   "source": [
    "# Get an output tensor for each of your tests\n",
    "modelo.eval()\n",
    "\n",
    "output_tensors = modelo(X_test)\n",
    "\n",
    "print(\"Tensor de salida para los primeros ejemplos de prueba:\")\n",
    "print(output_tensors[:2])\n",
    "\n",
    "_, predicted_categories = torch.max(output_tensors, 1)\n",
    "\n",
    "print(\"Categorías predichas para los primeros ejemplos de prueba:\")\n",
    "print(predicted_categories[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "09272b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1302])\n",
      "torch.Size([1302, 1962])\n",
      "torch.Size([1302])\n"
     ]
    }
   ],
   "source": [
    "print(predicted_categories.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e838c-3cab-4b6f-b041-1fde6a6d29aa",
   "metadata": {},
   "source": [
    "2. What does each value in the tensor represents?\n",
    "    - Cada valor en el tensor `X_one_hot_float` es parte de una representacion one-hot. Si un valor es 1.0, indica que la categoría correspondiente está presente en esa posición específica. Si el valor es 0.0, indica que la categoría no está presente. El vector `y` son las categorias que la red neuronal va a clasificar con respecto a los `X_one_hot_float`.\n",
    "    - Cada uno de las respuestas del tensor de salida de la red, representa a la letra siguiente predicha por la red neuronal, como se muestra a continuacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0a434e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'him',\n",
       " 'forget',\n",
       " 'was',\n",
       " 'to',\n",
       " 'in',\n",
       " 'village',\n",
       " 'as',\n",
       " 'he',\n",
       " ',',\n",
       " 'long',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'he',\n",
       " ',',\n",
       " 'children',\n",
       " 'the',\n",
       " ',',\n",
       " 'will',\n",
       " 'he',\n",
       " 'had',\n",
       " ',',\n",
       " 'hand',\n",
       " ',',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'he',\n",
       " ',',\n",
       " 'children',\n",
       " 'and',\n",
       " ',',\n",
       " 'had',\n",
       " 'the',\n",
       " 'in',\n",
       " 'had',\n",
       " 'with',\n",
       " 'of',\n",
       " 'into',\n",
       " 'the',\n",
       " 'long',\n",
       " ',',\n",
       " 'Arcadio',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'to',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'own',\n",
       " 'to',\n",
       " 'he',\n",
       " 'village',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'long',\n",
       " 'in',\n",
       " 'Buendia',\n",
       " ',',\n",
       " 'the',\n",
       " 'that',\n",
       " 'bearings.”',\n",
       " ',',\n",
       " 'by',\n",
       " 'and',\n",
       " 'of',\n",
       " 'to',\n",
       " 'long',\n",
       " 'the',\n",
       " 'a',\n",
       " 'and',\n",
       " 'During',\n",
       " 'the',\n",
       " 'was',\n",
       " ',',\n",
       " 'with',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'to',\n",
       " 'in',\n",
       " 'and',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Arcadio',\n",
       " 'enormous',\n",
       " ',',\n",
       " 'the',\n",
       " 'long',\n",
       " 'long',\n",
       " 'he',\n",
       " ',',\n",
       " ',',\n",
       " 'gypsy',\n",
       " ',',\n",
       " 'hand',\n",
       " 'his',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'had',\n",
       " 'Arcadio',\n",
       " ',',\n",
       " 'and',\n",
       " 'Colonel',\n",
       " ',',\n",
       " 'and',\n",
       " 'five',\n",
       " 'their',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'to',\n",
       " 'time',\n",
       " 'by',\n",
       " 'the',\n",
       " 'to',\n",
       " 'was',\n",
       " 'he',\n",
       " 'time',\n",
       " 'was',\n",
       " 'planets',\n",
       " 'hand',\n",
       " 'village',\n",
       " ',',\n",
       " 'hand',\n",
       " 'the',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'months',\n",
       " 'hand',\n",
       " ',',\n",
       " 'the',\n",
       " 'had',\n",
       " 'it.',\n",
       " 'village',\n",
       " 'had',\n",
       " ',',\n",
       " ',',\n",
       " 'back',\n",
       " 'testimony',\n",
       " ',',\n",
       " 'and',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " 'Buendia',\n",
       " 'had',\n",
       " 'time',\n",
       " 'be',\n",
       " ',',\n",
       " 'to',\n",
       " 'of',\n",
       " 'the',\n",
       " ',',\n",
       " 'long',\n",
       " 'and',\n",
       " 'reales',\n",
       " 'the',\n",
       " 'Jose',\n",
       " 'been',\n",
       " 'own',\n",
       " 'he',\n",
       " 'had',\n",
       " 'he',\n",
       " ',',\n",
       " 'by',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'village',\n",
       " 'the',\n",
       " 'was',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'only',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'in',\n",
       " ',',\n",
       " 'long',\n",
       " 'village',\n",
       " ',',\n",
       " 'long',\n",
       " 'of',\n",
       " 'the',\n",
       " 'to',\n",
       " 'and',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'village',\n",
       " 'stories.',\n",
       " 'the',\n",
       " 'was',\n",
       " 'He',\n",
       " 'and',\n",
       " 'that',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'long',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'village',\n",
       " 'According',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'fabulous',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'been',\n",
       " 'the',\n",
       " ',',\n",
       " 'from',\n",
       " 'the',\n",
       " 'Arcadio',\n",
       " 'be',\n",
       " 'the',\n",
       " ',',\n",
       " 'hand',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'of',\n",
       " 'had',\n",
       " 'his',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'was',\n",
       " 'village',\n",
       " ',',\n",
       " 'Arcadio',\n",
       " 'the',\n",
       " 'he',\n",
       " 'that',\n",
       " 'he',\n",
       " 'and',\n",
       " 'into',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'the',\n",
       " 'had',\n",
       " 'months',\n",
       " 'village',\n",
       " 'attention',\n",
       " ',',\n",
       " 'in',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'the',\n",
       " 'and',\n",
       " 'village',\n",
       " 'been',\n",
       " 'and',\n",
       " 'had',\n",
       " 'long',\n",
       " 'attention',\n",
       " 'the',\n",
       " 'not',\n",
       " 'the',\n",
       " 'the',\n",
       " 'about',\n",
       " 'the',\n",
       " ',',\n",
       " 'hand',\n",
       " ',',\n",
       " ',',\n",
       " 'long',\n",
       " 'to',\n",
       " 'that',\n",
       " 'and',\n",
       " 'at',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " 'the',\n",
       " 'village',\n",
       " 'the',\n",
       " 'the',\n",
       " 'village',\n",
       " 'the',\n",
       " 'than',\n",
       " ',',\n",
       " 'be',\n",
       " 'not',\n",
       " ',',\n",
       " 'had',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'of',\n",
       " 'of',\n",
       " 'in',\n",
       " 'enormous',\n",
       " ',',\n",
       " 'was',\n",
       " 'of',\n",
       " 'under',\n",
       " 'attention',\n",
       " 'hand',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " 'village',\n",
       " 'up',\n",
       " 'village',\n",
       " 'hand',\n",
       " ',',\n",
       " 'into',\n",
       " 'together',\n",
       " 'to',\n",
       " 'and',\n",
       " 'the',\n",
       " ',',\n",
       " 'seemed',\n",
       " 'of',\n",
       " ',',\n",
       " 'not',\n",
       " 'by',\n",
       " 'been',\n",
       " 'discovery',\n",
       " 'and',\n",
       " 'had',\n",
       " 'long',\n",
       " ',',\n",
       " 'March',\n",
       " 'had',\n",
       " 'the',\n",
       " 'the',\n",
       " 'had',\n",
       " ',',\n",
       " ',',\n",
       " 'five',\n",
       " ',',\n",
       " 'and',\n",
       " 'hand',\n",
       " 'had',\n",
       " 'the',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'was',\n",
       " 'village',\n",
       " 'the',\n",
       " 'village',\n",
       " 'of',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'long',\n",
       " 'the',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'of',\n",
       " 'and',\n",
       " ',',\n",
       " 'the',\n",
       " 'he',\n",
       " 'about',\n",
       " 'an',\n",
       " 'of',\n",
       " 'the',\n",
       " 'had',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " 'village',\n",
       " 'imagination',\n",
       " 'village',\n",
       " 'the',\n",
       " 'to',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'be',\n",
       " ',',\n",
       " ',',\n",
       " 'he',\n",
       " 'the',\n",
       " 'more',\n",
       " ',',\n",
       " 'the',\n",
       " 'of',\n",
       " 'been',\n",
       " 'the',\n",
       " 'of',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'been',\n",
       " 'the',\n",
       " 'only',\n",
       " 'by',\n",
       " 'to',\n",
       " 'village',\n",
       " 'of',\n",
       " 'more',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " 'the',\n",
       " 'he',\n",
       " ',',\n",
       " 'own',\n",
       " ',',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " 'the',\n",
       " 'been',\n",
       " 'forever',\n",
       " ',',\n",
       " 'to',\n",
       " 'the',\n",
       " 'water',\n",
       " ',',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'of',\n",
       " ',',\n",
       " 'village',\n",
       " 'he',\n",
       " 'Arcadio',\n",
       " 'he',\n",
       " 'to',\n",
       " 'of',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'had',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'enormous',\n",
       " 'of',\n",
       " 'not',\n",
       " 'Arcadio',\n",
       " 'not',\n",
       " 'side',\n",
       " 'about',\n",
       " 'from',\n",
       " 'of',\n",
       " 'of',\n",
       " 'had',\n",
       " 'the',\n",
       " 'been',\n",
       " 'of',\n",
       " 'he',\n",
       " 'the',\n",
       " 'have',\n",
       " 'he',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'enormous',\n",
       " 'and',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'from',\n",
       " 'been',\n",
       " 'been',\n",
       " 'the',\n",
       " 'magic',\n",
       " 'the',\n",
       " 'he',\n",
       " 'about',\n",
       " 'a',\n",
       " 'magnetized',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " 'he',\n",
       " 'an',\n",
       " 'had',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'been',\n",
       " 'and',\n",
       " ',',\n",
       " ',',\n",
       " 'that',\n",
       " 'long',\n",
       " 'long',\n",
       " 'had',\n",
       " 'the',\n",
       " 'the',\n",
       " 'had',\n",
       " ',',\n",
       " 'enormous',\n",
       " ',',\n",
       " 'and',\n",
       " 'village',\n",
       " 'to',\n",
       " ',',\n",
       " 'fabulous',\n",
       " 'village',\n",
       " ',',\n",
       " 'time',\n",
       " 'and',\n",
       " 'the',\n",
       " 'been',\n",
       " 'to',\n",
       " ',',\n",
       " 'of',\n",
       " 'to',\n",
       " 'had',\n",
       " ',',\n",
       " 'was',\n",
       " 'and',\n",
       " 'long',\n",
       " 'and',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'and',\n",
       " 'had',\n",
       " 'the',\n",
       " ',',\n",
       " 'with',\n",
       " ',',\n",
       " 'the',\n",
       " '“Right',\n",
       " ',',\n",
       " 'been',\n",
       " ',',\n",
       " 'let',\n",
       " 'the',\n",
       " ',',\n",
       " 'be',\n",
       " 'him',\n",
       " 'long',\n",
       " 'from',\n",
       " 'to',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'long',\n",
       " ',',\n",
       " 'had',\n",
       " 'he',\n",
       " 'Buendia',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " '“We',\n",
       " ',',\n",
       " 'of',\n",
       " 'Buendia',\n",
       " 'was',\n",
       " 'to',\n",
       " 'long',\n",
       " 'the',\n",
       " 'hand',\n",
       " 'was',\n",
       " ',',\n",
       " ',',\n",
       " 'he',\n",
       " 'has',\n",
       " 'the',\n",
       " 'and',\n",
       " 'long',\n",
       " 'had',\n",
       " 'village',\n",
       " 'of',\n",
       " 'the',\n",
       " 'own',\n",
       " ',',\n",
       " 'time',\n",
       " 'the',\n",
       " 'Melquiades’',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'and',\n",
       " 'and',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'he',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " 'by',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'had',\n",
       " ',',\n",
       " 'and',\n",
       " 'had',\n",
       " 'the',\n",
       " 'village',\n",
       " 'that',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'children',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'was',\n",
       " 'was',\n",
       " 'the',\n",
       " 'and',\n",
       " 'that',\n",
       " 'he',\n",
       " ',',\n",
       " 'village',\n",
       " ',',\n",
       " 'of',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " 'became',\n",
       " 'planets',\n",
       " 'the',\n",
       " 'the',\n",
       " 'and',\n",
       " 'him',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'of',\n",
       " ',',\n",
       " 'village',\n",
       " 'Buendia',\n",
       " ',',\n",
       " 'were',\n",
       " 'village',\n",
       " 'of',\n",
       " 'hand',\n",
       " 'away',\n",
       " 'had',\n",
       " 'to',\n",
       " 'was',\n",
       " 'of',\n",
       " 'village',\n",
       " 'of',\n",
       " 'village',\n",
       " 'and',\n",
       " 'long',\n",
       " 'on',\n",
       " ',',\n",
       " 'long',\n",
       " ',',\n",
       " ',',\n",
       " 'imagination',\n",
       " 'time',\n",
       " 'up',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " ',',\n",
       " 'and',\n",
       " 'sin',\n",
       " 'and',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'and',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " ',',\n",
       " 'for',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'village',\n",
       " 'the',\n",
       " 'had',\n",
       " 'village',\n",
       " 'village',\n",
       " 'was',\n",
       " 'of',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'like',\n",
       " 'of',\n",
       " 'had',\n",
       " 'hand',\n",
       " 'the',\n",
       " 'enormous',\n",
       " ',',\n",
       " 'roof',\n",
       " 'the',\n",
       " 'village',\n",
       " 'Arcadio',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " 'magic',\n",
       " ',',\n",
       " 'be',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'up',\n",
       " 'until',\n",
       " 'the',\n",
       " 'village',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'Arcadio',\n",
       " 'Arcadio',\n",
       " ',',\n",
       " 'to',\n",
       " 'in',\n",
       " 'not',\n",
       " 'was',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'a',\n",
       " 'be',\n",
       " 'more',\n",
       " ',',\n",
       " 'he',\n",
       " 'village',\n",
       " 'the',\n",
       " 'made',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'him',\n",
       " 'imagination',\n",
       " 'Jose',\n",
       " ',',\n",
       " 'the',\n",
       " 'village',\n",
       " 'to',\n",
       " 'to',\n",
       " 'the',\n",
       " 'the',\n",
       " ',',\n",
       " 'village',\n",
       " 'the',\n",
       " 'the',\n",
       " 'memory',\n",
       " 'long',\n",
       " 'swamp',\n",
       " 'village',\n",
       " 'with',\n",
       " 'of',\n",
       " 'to',\n",
       " 'years',\n",
       " 'and',\n",
       " 'of',\n",
       " 'village',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'of',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'of',\n",
       " 'magical',\n",
       " 'he',\n",
       " ',',\n",
       " 'the',\n",
       " 'him',\n",
       " 'hand',\n",
       " 'with',\n",
       " 'of',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'and',\n",
       " ',',\n",
       " 'enormous',\n",
       " 'the',\n",
       " 'and',\n",
       " 'of',\n",
       " ',',\n",
       " 'hands',\n",
       " ',',\n",
       " ',',\n",
       " 'village',\n",
       " 'Buendia',\n",
       " 'been',\n",
       " ',',\n",
       " ',',\n",
       " 'he',\n",
       " 'the',\n",
       " 'long',\n",
       " ',',\n",
       " 'about',\n",
       " 'that',\n",
       " 'he',\n",
       " 'was',\n",
       " 'village',\n",
       " 'the',\n",
       " 'the',\n",
       " 'they',\n",
       " 'the',\n",
       " 'Buendia',\n",
       " 'own',\n",
       " 'village',\n",
       " 'having',\n",
       " 'up',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'he',\n",
       " 'the',\n",
       " ',',\n",
       " 'to',\n",
       " 'about',\n",
       " ',',\n",
       " 'village',\n",
       " 'long',\n",
       " 'long',\n",
       " 'more',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'the',\n",
       " 'of',\n",
       " 'village',\n",
       " 'a',\n",
       " 'and',\n",
       " 'village',\n",
       " 'village',\n",
       " 'and',\n",
       " 'the',\n",
       " 'were',\n",
       " 'village',\n",
       " 'of',\n",
       " ',',\n",
       " 'had',\n",
       " 'the',\n",
       " 'maps',\n",
       " 'the',\n",
       " 'hand',\n",
       " 'of',\n",
       " 'had',\n",
       " 'Arcadio',\n",
       " 'and',\n",
       " 'had',\n",
       " 'of',\n",
       " ',',\n",
       " 'and',\n",
       " ',',\n",
       " ',',\n",
       " 'concerning',\n",
       " 'had',\n",
       " 'village',\n",
       " 'the',\n",
       " 'hand',\n",
       " 'about',\n",
       " 'the',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " 'was',\n",
       " 'and',\n",
       " 'and',\n",
       " ',',\n",
       " 'been',\n",
       " 'he',\n",
       " 'the',\n",
       " 'village',\n",
       " 'of',\n",
       " 'and',\n",
       " 'of',\n",
       " ',',\n",
       " 'about',\n",
       " ',',\n",
       " ',',\n",
       " ',',\n",
       " 'the',\n",
       " ',',\n",
       " 'the',\n",
       " 'long',\n",
       " ',',\n",
       " '“We’ll',\n",
       " 'the',\n",
       " 'age',\n",
       " 'be',\n",
       " 'and',\n",
       " 'the',\n",
       " ',',\n",
       " 'sigh',\n",
       " 'inhabitants.',\n",
       " 'he',\n",
       " ',',\n",
       " ',',\n",
       " 'of',\n",
       " 'every',\n",
       " 'the',\n",
       " 'village',\n",
       " 'children',\n",
       " 'had',\n",
       " 'and',\n",
       " ',',\n",
       " 'the',\n",
       " 'the',\n",
       " 'been',\n",
       " 'had',\n",
       " 'he',\n",
       " 'had',\n",
       " 'him',\n",
       " 'and',\n",
       " 'village',\n",
       " 'that',\n",
       " ...]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[itot.get(w2, w2) for w2 in predicted_categories.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eb8645",
   "metadata": {},
   "source": [
    "3. Why does it make sense to choose that number of neurons in our layer?\n",
    "    - Tiene sentido elegir el número de neuronas con respecto a el numero de tokens unicos (categorias), porque queremos que cada neurona en la capa de entrada corresponda a una categoría en los datos. Esto asegura que toda la información categórica esté correctamente representada en el dominio del vocabulario a usar en el entrenamiento y test del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc58f3",
   "metadata": {},
   "source": [
    "4. What's the negative likelihood for each example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d8f3c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log Likelihood (NLL) para el conjunto de entrenamiento: \n",
      "para el vector de entrenamiento: tensor([[-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        ...,\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "para el conjunto de entrenamiento: 7.146196365356445\n",
      "\n",
      "Negative Log Likelihood (NLL) para el conjunto de prueba: \n",
      "para el vector de prueba: tensor([[-7.5825, -7.5825, -7.5825,  ..., -7.5825, -7.5825, -7.5825],\n",
      "        [-7.5823, -7.5823, -7.5823,  ..., -7.5823, -7.5823, -7.5823],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826],\n",
      "        ...,\n",
      "        [-7.5820, -7.5822, -7.5820,  ..., -7.5820, -7.5822, -7.5817],\n",
      "        [-7.5820, -7.5822, -7.5820,  ..., -7.5820, -7.5822, -7.5817],\n",
      "        [-7.5826, -7.5826, -7.5826,  ..., -7.5826, -7.5826, -7.5826]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "para el conjunto de prueba: 7.434174060821533\n"
     ]
    }
   ],
   "source": [
    "outputs = modelo(X_train)\n",
    "\n",
    "log_probs = torch.log_softmax(outputs, dim=1)\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "nll = nll_loss(log_probs, y_train)\n",
    "\n",
    "print('Negative Log Likelihood (NLL) para el conjunto de entrenamiento: ')\n",
    "print(f\"para el vector de entrenamiento: {log_probs}\")\n",
    "print(f\"para el conjunto de entrenamiento: {nll.item()}\")\n",
    "\n",
    "outputs = modelo(X_test)\n",
    "\n",
    "log_probs = torch.log_softmax(outputs, dim=1)\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "nll = nll_loss(log_probs, y_test)\n",
    "\n",
    "print('\\nNegative Log Likelihood (NLL) para el conjunto de prueba: ')\n",
    "print(f\"para el vector de prueba: {log_probs}\")\n",
    "print(f\"para el conjunto de prueba: {nll.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539bbe5",
   "metadata": {},
   "source": [
    "5. Try generating a few sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e370d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.eval()\n",
    "tokens_init = ['I','Aureliano','MANY']\n",
    "sentences = []\n",
    "for token_init in tokens_init:\n",
    "    seed_init= ttoi[token_init]\n",
    "    sentence = [token_init]\n",
    "    for _ in range(20):\n",
    "        X_word = X_one_hot[seed_init]\n",
    "        y_word = modelo(X_word.reshape(1, len(X_word)))\n",
    "        _, predicted_word = torch.max(y_word, 1)\n",
    "        new_word= itot[predicted_word.item()]\n",
    "        sentence.append(new_word)\n",
    "        seed_init = predicted_word.item()\n",
    "    sentences.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "900d6260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I the , he hand from the , he hand from the , he hand from the , he hand from',\n",
       " 'Aureliano to , he hand from the , he hand from the , he hand from the , he hand from',\n",
       " 'MANY children kept , he hand from the , he hand from the , he hand from the , he hand']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca76773",
   "metadata": {},
   "source": [
    "6. What's the negative likelihood for each sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d7831d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll = []\n",
    "for sentence in sentences:\n",
    "    sentence_i  = [ttoi[word] for word in sentence.split()]\n",
    "    nll.append(torch.log_softmax(torch.tensor(sentence_i, dtype=torch.long).float().reshape(-1,len(sentence_i)), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b3b605ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.6564e+03, -1.3863e+00, -1.7034e+03, -8.7839e+02, -8.9539e+02,\n",
       "          -9.6139e+02, -1.3863e+00, -1.7034e+03, -8.7839e+02, -8.9539e+02,\n",
       "          -9.6139e+02, -1.3863e+00, -1.7034e+03, -8.7839e+02, -8.9539e+02,\n",
       "          -9.6139e+02, -1.3863e+00, -1.7034e+03, -8.7839e+02, -8.9539e+02,\n",
       "          -9.6139e+02]]),\n",
       " tensor([[-1719.,     0., -1738.,  -913.,  -930.,  -996.,   -36., -1738.,  -913.,\n",
       "           -930.,  -996.,   -36., -1738.,  -913.,  -930.,  -996.,   -36., -1738.,\n",
       "           -913.,  -930.,  -996.]]),\n",
       " tensor([[-1.6401e+03, -1.3261e+03, -7.2610e+02, -1.7031e+03, -8.7810e+02,\n",
       "          -8.9510e+02, -9.6110e+02, -1.0986e+00, -1.7031e+03, -8.7810e+02,\n",
       "          -8.9510e+02, -9.6110e+02, -1.0986e+00, -1.7031e+03, -8.7810e+02,\n",
       "          -8.9510e+02, -9.6110e+02, -1.0986e+00, -1.7031e+03, -8.7810e+02,\n",
       "          -8.9510e+02]])]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e533cf-33b0-4a61-84cb-57c5793893ee",
   "metadata": {},
   "source": [
    "### Design your own neural network (more layers and different number of neurons)\n",
    "The goal is to get sentences that make more sense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d3136a66-c972-41be-83bd-9a573a44df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class secondNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(secondNet, self).__init__()\n",
    "        # Agregando más capas\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)  # Primera capa oculta\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)  # Segunda capa oculta\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)  # Capa de salida\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pasando los datos por las capas y activaciones\n",
    "        out = F.relu(self.fc1(x))  # Activación ReLU después de la primera capa\n",
    "        out = F.relu(self.fc2(out))  # ReLU después de la segunda capa\n",
    "        out = self.fc3(out)  # Capa de salida sin activación\n",
    "        out = F.softmax(out, dim=1)  # Aplicar softmax en la salida\n",
    "        return out\n",
    "\n",
    "# Cambiar input_size, hidden_size1, hidden_size2 y output_size según lo que necesites\n",
    "modelo_nn = secondNet(input_size=n_tokens, hidden_size1=n_tokens*3, hidden_size2=n_tokens*2, output_size=n_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "07d00a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(modelo_nn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "79039d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 7.3905\n",
      "Epoch [20/100], Loss: 7.3769\n",
      "Epoch [30/100], Loss: 7.3755\n",
      "Epoch [40/100], Loss: 7.3743\n",
      "Epoch [50/100], Loss: 7.3734\n",
      "Epoch [60/100], Loss: 7.3730\n",
      "Epoch [70/100], Loss: 7.3730\n",
      "Epoch [80/100], Loss: 7.3730\n",
      "Epoch [90/100], Loss: 7.3730\n",
      "Epoch [100/100], Loss: 7.3730\n",
      "Entrenamiento completado\n"
     ]
    }
   ],
   "source": [
    "modelo_nn.train()\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = modelo_nn(X_train)\n",
    "    \n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "print(\"Entrenamiento completado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9a34fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_nn.eval()\n",
    "tokens_init = ['Before','future','Many']\n",
    "sentences = []\n",
    "for token_init in tokens_init:\n",
    "    seed_init= ttoi[token_init]\n",
    "    sentence = [token_init]\n",
    "    for _ in range(20):\n",
    "        X_word = X_one_hot[seed_init]\n",
    "        y_word = modelo_nn(X_word.reshape(1, len(X_word)))\n",
    "        _, predicted_word = torch.max(y_word, 1)\n",
    "        new_word= itot[predicted_word.item()]\n",
    "        sentence.append(new_word)\n",
    "        seed_init = predicted_word.item()\n",
    "    sentences.append(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d437301d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Before the , the , the , the , the , the , the , the , the , the ,',\n",
       " 'future to of village the , the , the , the , the , the , the , the , the',\n",
       " 'Many to of village the , the , the , the , the , the , the , the , the']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
